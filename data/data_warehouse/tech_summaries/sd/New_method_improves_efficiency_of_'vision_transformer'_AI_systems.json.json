{
    "unique_id": "39a72cb7-ff26-5be5-815d-2319c8e3f7c5",
    "title": "New method improves efficiency of 'vision transformer' AI systems",
    "summary": "Researchers have developed a new methodology called \"Patch-to-Cluster attention\" (PaCa) to address challenges faced by Vision Transformers (ViTs). ViTs, which are transformer-based AI models trained on visual inputs, require significant computational power and memory due to the complexity of transformer models. Additionally, understanding the decision-making process of ViTs is difficult. PaCa addresses these challenges by using clustering techniques to reduce computational demands and improve model interpretability. Comprehensive testing showed that PaCa outperformed other ViTs in object classification, identification, and segmentation tasks. The next step is to scale up PaCa by training on larger datasets.",
    "link": "https://www.sciencedaily.com/releases/2023/06/230601160053.htm",
    "published": "2023-06-01"
}
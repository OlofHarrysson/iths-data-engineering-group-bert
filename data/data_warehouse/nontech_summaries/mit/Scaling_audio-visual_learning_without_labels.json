{
    "unique_id": "9ac18b23-9bd2-52a8-bd3f-8b25698b5d70",
    "title": "Scaling audio-visual learning without labels",
    "summary": "Researchers from MIT, the MIT-IBM Watson AI Lab, IBM Research, and other institutions have developed a new technique for analyzing unlabeled audio and visual data. The technique, called the contrastive audio-visual masked autoencoder (CAV-MAE), combines two architectures of self-supervised learning to improve the performance of machine-learning models used in applications like speech recognition and object detection. The CAV-MAE method uses large datasets of audio and video clips to train a neural network to extract meaningful representations from the data. The researchers found that CAV-MAE outperformed previous techniques and could be used for tasks like audio-visual retrieval and event classification. The researchers believe that this technique could be applied to other modalities in the future.",
    "link": "https://news.mit.edu/2023/scaling-audio-visual-learning-without-labels-0605",
    "published": "2023-06-05"
}
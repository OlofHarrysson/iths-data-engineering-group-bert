{
    "unique_id": "061a81a1-b472-50d2-9423-5537925e4175",
    "title": "GPT-3.5 Turbo fine-tuning and API updates",
    "summary": "OpenAI has announced the availability of fine-tuning for GPT-3.5 Turbo, with fine-tuning for GPT-4 coming in the fall. This update allows developers to customize models for their specific use cases and run them at scale. Early tests have shown that a fine-tuned version of GPT-3.5 Turbo can match or even outperform base GPT-4 capabilities on certain tasks. The data sent in and out of the fine-tuning API is owned by the customer and not used by OpenAI or any other organization. Fine-tuning enables improved steerability, reliable output formatting, and custom tone. It also allows for shorter prompts and can handle 4k tokens. Fine-tuning is most effective when combined with other techniques such as prompt engineering, information retrieval, and function calling. OpenAI will be introducing a fine-tuning UI in the future. Safety is a priority, and fine-tuning training data goes through a moderation system to detect unsafe content. The pricing for fine-tuning includes training costs and usage costs. OpenAI has also announced the availability of babbage-002 and davinci-002 as replacements for the original GPT-3 base models. The old fine-tuning endpoint will be turned off on January 4th, 2024.",
    "link": "https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates",
    "published": "2023-08-22"
}